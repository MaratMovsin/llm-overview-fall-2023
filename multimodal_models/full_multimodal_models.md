## Intro to Multimodal Models
_"... a large gap persists between the capabilities of LLMs and true humanlike intelligence. 
This is partially because humans perceive a variety of sensory inputs while LLMs are typically
restricted to Language..."_ - Microsoft Azure Cognitive Services Research (MACSR)

"_The convergence of text, visual, and audio data is a key step towards human-like
artificial intelligence_" - MACSR.
### Selling Multimodal

#### Ilya Sutskever, OpenAI chief scientist
В [недавнем интервью](https://www.nvidia.com/en-us/on-demand/session/gtcspring23-s52092/?ncid=so-yout-561702)
Jensen Huang Илья рассказал про свое видение Multimodal направления развития ML. 
Две причины почему расширение LLM на другие модальности (в основном vision) кажется Илье интересным:

 - Humble: это просто полезно.
   Мир, как его воспринимает человек, очень визуален. 
   Поэтому полезность нейросеток без визуальной модальности не такая большая, 
   как могла бы быть если бы сетки видели то, что видят люди. 
   Кстати, в GPT-4 добавили vision, теперь это мультимодальная модель.
 - Not as clear-cut as it may seem: дополнительно учась на изображениях, 
   модель узнает и поймет больше, чем пользуясь одним текстом.
   Люди за всю жизнь слышат не более миллиарда слов, 
   и для людей важно получать информацию из всех доступных модальностей, 
   причем он считает, что мы выучиваем гораздо больше из визуальной модальности. 
   То же верно для сеток, с тем исключением, что сетки успевают увидеть триллионы слов, 
   поэтому им проще узнать много о мире из одного лишь текста. 
   Тем не менее, это неэффективно, 
   и можно гораздо быстрее понять мир пользуясь изображениями и видео.
   То есть важное преимущество, которое приносит Multimodal - 
   благодаря новым каналам информации
   становится проще обучиться понимать мир. А также приблизить понимание
   к человеческому.

Он приводит пример с GPT-4, которая начала решать математический тест 
c диаграммами в условиях сильно лучше после добавление Vision.
Илья говорит, что кроме понимания мира есть и другие аспекты:
иметь возможность рассуждать ("to reason") визуально и коммуницировать визуально -
это очень мощные вещи.
   
#### New capabilities
С Multimodal LLM-агенты становятся приспособлены к задачам, 
которые не удавалось качественно решить в Unimodal сеттинге. 
Пример: Document Editing.

<img src="assets/doc_editing.png" width="600">

[SOTA-решение](https://arxiv.org/pdf/2212.02623.pdf) (на 13 марта) 
обрабатывает документ по трем модальностям - visual, text и layout 
(разметка, тоже своего рода модальность). На вход подается изображение,
из него экстрагируется layout и text, после чего трансформер обучается на трех
связанных модальностях. Ожидаемо 
подход имеет преимущество перед наивным ViT с masked unit modelling.

### Состояние области в соответствии с недавним [обзором](https://arxiv.org/pdf/2309.10020.pdf)
Иллюстрация текущего положения дел из [обзорной статьи](https://arxiv.org/pdf/2309.10020.pdf) 
про Multimodal c упором на vision - параллель между прогрессом Textual и Multimodal моделей.

<img src='assets/models_compared.png' width='600'>

Главные компоненты задачи, выделенные в той статье (опять же с фокусом на vision)

<img src='assets/questions.png' width='600'>

И классификация vision-ориентированных моделей по функции

<img src='assets/paper_structure.png' width='600'>


### Central challenges

 - Data Collection and Labeling.
For instance, gathering a balanced set of user posts that contain varied data types – 
text, images, videos – is no small feat. Accurately labeling this data and making sure 
modalities are aligned is also typically harder than Unimodal data processing.
 - Model Complexity.
Multimodal learning models are inherently complex due to their need to process 
and integrate multiple data types.
 - Data Fusion. 
How and when to combine different modalities into one flow? Early fusion might cause loss of unique characteristics of individual modalities, 
while late fusion might not capture the correlations between modalities adequately.
Moreover, depending on the task you've got to find the right balance between the importance of 
modalities - e.g.
textual data (genres, tags), visual data (posters), and audio data (soundtracks, dialogue) 
for the recommendation task.

## i-Code-V1

Статья https://arxiv.org/abs/2205.01818, про i-Code-V1, была опубликована
5 мая 2022 года. На то время, по словам авторов, большинство методов предобучения
были ограничены одной или двумя размерностями. В статье был представлен i-Code – 
фреймворк для предобучения без учителя, пользователи которого могут 
гибко комбинировать компьютерное зрение, голосовую речь и естественные языки, и 
получать унифицированные представления (вектора) общего вида для любой комбинации входных данных.

В данном фреймворке данные каждой размерности вначале подаются на вход в соответствующий
предобученный одноразмерный энкодер, после чего интегрируются в мультимодальную сеть слияния, которая
использует новые механизмы внимания и другие иновационные особенности архитектуры. 
Вся система проходит сквозное предобучение с новой целевой функцией потерь, 
включая модуль маскирования размерности, и мультиразмерного контрастного обучения.

i-Code может динамически обрабатывать одно, два и три-размерные данные во время обучения и инференса.
Экспериментальные результаты демонстрируют, как i-Code может превзойти самые современные (на момент написания статьи)
методы в пяти задачах на понимание видео и бенчмарке GLUE NLP, улучшая качество на целых 11%
и демонстрируя мощь интегративного мультимодального предобучения. 

### Многоразмерные мультимодальные данные для предобучения

Для эффективного мультимодального обучения авторы собрали много данных для предобучения.
Они использовали два типа данных: видео, в которых есть все три размерности и двухмодальные наборы данных.

YT-Temporal-180M был выбран для представления видео с тремя модальностями: видео, речь и текст. 
Этот набор данных состоит из 180 миллионов видеоклипов, и для каждого видеоклипа авторы выбрали 8 кадров в качестве визуальных входных данных. 
Данные речи были извлечены из аудиофрагментов и прошли дальнейшую обработку. К каждому клипу также приложены субтитры, которые 
и использовались в качетсве текстовых данных. 

Помимо видео, использовались и двухмодальные данные. Florence computer vision foundation model (Yuan et al., 2021) 
содержит 72,8 миллионов пар изображений и текстовых описаний, а набор данных язык-речь содержит 75 тысяч часов 
английской речи с соответствующими транскрипциями. Для зрения-текста использовали датасет Spoken Moments in
Time (SMiT), состоящий из 500k содержит 500 тысяч озвученных титров, каждый из
которых изображает широкий спектр различных событий в коротком видеоролике.

Это первый случай, когда парные наборы данных использовались для обучения
моделей "зрение-язык-речь".
Такое объединение различных типов данных помогло улучшить производительность модели в мультимодальном обучении.

### Архитектура модели

Архитектура модели i-Code состоит из четырех модулей. Первые три модуля - это кодировщики отдельных модальностей (видео, язык и речь). 
Последний модуль - модуль слияния модальностей. Для каждой модальности мы подаем входные данные в соответствующий кодировщик, 
затем закодированные данные проходят через линейный и объединяются в модуле слияния модальностей.

Вместо того чтобы обучать каждый кодировщик модальности с нуля, авторы разработали модульную структуру, 
позволяющую использовать предобученные модели для каждой модальности. Это обеспечивает модулю слияния высококачественные
контекстные представления для более эффективного мультимодального понимания. 

Для каждой модальности используюстя state-of-the-art модели:

* Для кодировщика языка – DeBERTa V3 base (He et al., 2020), которая обладает разделенным механизмом внимания и достигла
выдающихся результатов на бенчмарках GLUE и SuperGLUE.
* Для кодировщика видео авторы адоптировали модель CoSwin Transformer (Yuan et al., 2021). Чтобы обеспечить возможность 
обработки как изображений, так и последовательности кадров (видео), создается экземпляр видео CoSwin Transformer из
предобученной модели CoSwin Transformer, следуя процедуре, описанной в работе Liu et al. (2021b). 
CoSwin Transformer содержит 91 миллион параметров.
* Для кодировщика речи используется предобученную версия WavLM-large (Chen et al., 2021), 315 миллионов параметров.
Модель использует темпоральный сверточный энкодер для извлечения признаков из входных речевых данных (звуковая волна) и
и трансформерный энкодер для дальнейшей обработки.

#### Модуль слияния модальностей

Характеристики, извлеченные каждым кодировщиком одной модальности, проецируются в скрытый слой сети слияния с помощью 
однослойной сети прямого распростронения. Полученные признаки (вектора) подаются на вход сети слияния модальностей для 
создания интегративных мультимодальных представлений. Поскольку позиционная информация уже включена в кодировщики одиночной 
модальности, авторы не используют позиционные эмбеддинги в модуле слияния.

Основой сети слияния является трансформерный энкодер, где каждый слой выполняет перекрестное внимание между модальностями, 
прямую проекцию и нормализацию слоя. Для облегчения более эффективного понимания разных модальностей мы исследуются две 
вариации традиционного механизма внимания: merge-attention и co-attention, как показано на рис. 1.

![model_architecture.png](assets/model_architecture.png)
> Рис. 1. Слева: Общая архитектура модели i-Code. Справа: слои внимания и прямого распростронения в слое сети слияния с (a) 
> merge-attention и (b) co-attention. Для упрощения для модальности естественного языка изображены только остаточные соединения.

##### Merge-attention

В этой конфигурации для разных модальностей используются одни и те же параметры внимания. Чтобы помочь 
модулю слияния различать разные модальности, к проецированным характеристикам 
добавляется уникальный идентификатор для каждой модальности. Полученные эмбеддинги из разных модальностей объединяются 
вместе и подаются на вход сети слияния, где каждый слойповторяет слои классического энкодера трансформера (Vaswani et al., 2017).

##### Co-attention

В этой конфигурации каждый слой трансформера включает вначале слой самовнимания вместе 
с уникальными параметрами для каждой модальности.

### Предобучение

#### Masked Language Modeling

Маскирование языка показало хорошие результаты как в языковых моделях(Devlin et al., 2019), так и в моделях язык-зрение(Dou et al., 2021).
Во время предобучения в i-Code маскируется 30% текстовых токенов аналогично с предобучением BERT. Цель – предсказать 
маскированные токены; функция потерь – крсс-энтропия между исходными и предсказанными токенами.

#### Masked Vision Modeling

Для предобучения зрения авторы преобразуют входные визуальные данные в дискретные токены, затем маскируют куски входных 
изображений и максимизируют кросс-энтропию между исходными и предсказанными токенами маскированного языка из кусков изображений.
Для дискретизации последовательности фреймов используется PeCo
(Dong et al., 2021). Для маскирования применяется стратегия 3D tube-masking
(Wang et al., 2021) с коэффициентом маскирования патча равным 50%. 

#### Masked Span Modeling

Для дискретизации речевых данных в последовательность токенов используется wav2vec 2.0 (Baevski et al., 2020). 
Используется такая же маскирующая стратегия как и в HuBERT (Hsu et al., 2021) и wav2vec 2.0 (Baevski
et al., 2020). Функция потерь – кросс-энтропия между исходными токенами и предсказанными. 

Про предобучение кросс-модальной части, а также про подробнсти экспериментов с разными вариантами можели
можно прочитать в оригинальной статье 
[i-Code: An Integrative and Composable Multimodal Learning Framework](https://arxiv.org/abs/2205.01818)

### Анализ результатов

![table.png](assets/table.png)

На таблице выше посчитана эффективность моделей в распознавании эмоций на CMU MOSEEI.
Авторы делают вывод, что речь является наиболее хорошим предиктором среди одиночных модальностей, что логично учитывая 
эмоциональные свойства человеческой речи (Peerzada et al., 2018).
При этом использование любого набора из двух размерностей эффективнее чем речь, а самой эффективной комбинацией является 
язык+речь. Использование всех трех уже не дает прироста по отношению к речи-языку.

Также авторы замечают, что предобученные на двухмодальных наборах данных модели выигрывают у предобученных на видео, 
где встречаются все три модальности. i-Code побеждает основные модели на 5 задачах понимания видео и GLUE NLP бенчмарке.

## I-Code V3(CoDi): Any-to-Any Generation via Composable Diffusion

Уже существуют генеративные модели, работающие с несколькими модальностями(чаще всего с текстом и изображениями). 
Однако авторы [статьи](https://arxiv.org/pdf/2305.11846.pdf) пошли дальше и предложили модель $CoDi$, 
способную обрабатывать и генерировать произвольные наборы модальностей. 

<img src="assets/CoDi.png" width="600">

До этого модели были ограничены требуемыми данными, которые должны содержать разные модальности и при этом 
соотноситься друг с другом. Например, нужно видео с аудиодорожкой и при этом должно быть некоторое текстовое описание для него. 
Очевидно, таких данных не много. А последовательно из одной модальности генерировать следующую - долго, 
дорого и качество такой генерации будет ограничено.

### Общая схема обучения

Во-первых, авторы обучили латентные диффузионные модели (LDM) для каждой модальности(текст, изображения, видео и аудио). 
Так получили качественные одномодальные генеративные модели. Для условной межмодальной 
генерации(например, по тексту генерировать аудио) входные модальности проецируются в общее пространство признаков. 
Такой механизм подготавливает диффузионную модель к обуславливанию на произвольный набор модальностей, без прямого обучения для этого. 

Во-вторых, добавляются модули cross-atetntion для каждого диффьюзера и environment encoder V для проекции 
латентных переменных разных LDM в общее пространство признаков. Далее замараживаются LDM. Поскольку V для каждого энкодера выравнены, LDM могут взаимодействовать с люым набором модальностей, интерполируя входные данные эмбеддингами из V.

<img src="assets/CoDi_stages.png" width="600">

### Composable Multimodal Conditioning

Одновременная оптимизация всех четырех кодеров подсказок комбинаторным способом требует больших вычислительных затрат при наличии $O(n_2)$ пар. Более того для некторых модальностей мало парных датасетов(например, изображение и аудио). Чтобы решить эту проблему, авторы предлажили метод "Bridging Alignment" для выравнивания энкодеров. В качестве связующей модальности выбран текст, так как с ним существует множество парных датасетов. 

### Composable Diffusion

#### Image Diffusion Model

Для энкодеров изобраений была выбрана архитектура Stable Diffusion 1.5 и инициализирована теми же весами. Повторное использование весов переносит знания и исключительную точность генерации Stable Diffusion 1.5, обученной на крупномасштабных наборах данных изображений высокого качества.

#### Video Diffusion Model

Чтобы моделировать временные свойства видео и одновременно поддерживать
качество генерации видения, создается видео diffuser, расширяя diffuser изображений временными модулями. В частности, используем псевдовременной механизм внимания перед residual блоком. Однако утверждается, что псевдовременное внимание позволяет видеокадрам глобально взаимодействовать друг с другом только путем выравнивания пикселей (размерность высоты, ширины), что приводит к
отсутствию межкадрового взаимодействия между локальными пикселями. Авторы утверждают, что это приводит к общей проблеме временной несогласованности расположения, формы и цвета при генерации видео. Для решения этой проблемы предлагается адаптировать метод скрытого
сдвига, который выполняет пространственно-временные сдвиги скрытых объектов в соответствии с временным
вниманием. Видео делится по скрытому измерению на k = 8 фрагментов, и для каждого фрагмента i = от 0 до 7 мы сдвигаем временное измерение вперед на i позиций. 

#### Audio Diffusion Model
Чтобы обеспечить гибкое переключение между модальностями при совместной генерации, аудиодиффузор спроектирован так, чтобы иметь архитектуру, аналогичную визуальным диффузорам, где mel-спектрограмму можно естественным образом рассматривать как изображение с 1 каналом. Используется энкодер VAE для кодирования мелспектрограммы звука в сжатое скрытое пространство. При синтезе звука декодер VAE сопоставляет скрытую
переменную с mel-спектрограммой, а vocoder генерирует аудиосэмпл из mel-спектрограммы.

#### Text Diffusion Model

VAE для текстового LDM является OPTIMUS, а его кодировщиком и декодером являются BERT и GPT-2 соответственно. Для денойзинга UNet, в отличие от UNet в image diffusion, $2D$ свертка в residual блоках заменяется одномерной сверткой.

### Joint Multimodal Generation by Latent Alignment

Заключительный шаг заключается в обеспечении перекрестного внимания между потоками диффузии при одновременной генерации двух или более модальностей. Это достигается добавлением подуровней перекрестного внимания к UNet θ. 
V (·) различных модальностей обучаются для выравнивания с помощью contrastive learning. Задача V - достигнуть генерации людых наборов модальностей. В итоге, CoDi может генерировать модальности A и B, хотя в данных были только пары A-C и D-C.

### Результаты

![Text to image](assets/CoDi_metrics1.png) ![Text to video](assets/CoDi_metrics2.png)

![Audio](assets/CoDi_metrics3.png)

![Any to image](assets/CoDi_metrics4.png)

![Text to video](assets/CoDi_metrics5.png)

## I-Code Studio: A Configurable and Composable Framework for Integrative AI

Интегративный ИИ является одним из важных направлений подхода
AGI, посредством объединения нескольких моделей для
решения сложных мультимодальных задач. В этой [статье](https://arxiv.org/pdf/2305.13738.pdf) Microsoft предлагает i-Code Studio, настраиваемый
и компонуемый фреймворк для интегративного AGI. I-Code Studio объединяет несколько предобученных моделей без fine-tuning'а для выполнения сложных мультимодальных задач. Вместо композиции моделей i-Code Studio предоставляет интегративную, гибкую и компонуемую среду, позволяющую разработчикам быстро и легко
создавать передовые сервисы и технологии, адаптированные к их конкретным требованиям.
I-Code Studio добивается впечатляющих результатов при
выполнении различных мультимодальных задач в zero-shot формате, таких как
преобразование видео в текст, перевод речи в речь и визуальные ответы на вопросы.

### The i-Code Studio Framework

Для сложных мультимодальных задач,
i-Code Studio предоставляет
разработчикам универсальную платформу для быстрой и простой интеграции и создания нескольких больших предварительно обученных моделей и сервисов
в различных модальностях без какого-либо обучения или
fine-tuning'а.

![Alt text](assets/Studio.png)

Для каждой задачи фреймворк может быть представлен
с помощью DAG, где узлы без входящего ребра
являются необработанными входными данными, такими как изображение, текст, видео
и речь, узлы без исходящих ребер являются
выходными данными данной задачи, а остальные
узлы являются базовыми моделями/сервисами или хранить промежуточные выходные данные модели из других моделей/сервисов.
Входные данные для узла поступают из необработанных входных данных
и/или выходных данных предыдущих узлов. Входные
данные проходят через каждый узел в DAG, позволяя
выполнять сложные мультимодальные задачи.
Исходящий ребро от узла модели/сервиса представляет
API, предоставляемый моделью/сервисом. Для каждой задачи, входные данные поступают в DAG из входных узлов
и обрабатываются одной или несколькими моделями или
сервисами моделей. 

![Alt text](assets/Studio2.png)

Для каждой задачи DAG настраивается таким образом, чтобы
связанные модели взаимодействовали для получения желаемого
результата. Различные компоненты i-Code Studio взаимодействуют, образуя единое интегрированное
решение и его можно настроить в соответствии с конкретными
потребностями пользователя. Например, для
задачи "Ответы на визуальные вопросы" (VQA) входными данными являются изображение и
вопрос, связанный с изображением (см. рис. 2).
Сначала мы можем применить к входному изображению службы субтитров и обнаружения объектов. Выходной текст,
содержащий визуальную информацию, объединяется
с вводимым вопросом в качестве запроса для ChatGPT,
который отвечает на вопрос. Для преобразования речи в речь
в DAG настроено распознавание речи
(SR) → Машинный перевод (MT) →
Преобразование текста в речь (TTS). Этот DAG расшифровывает
исходную речь, переводит транскрипцию на
целевой язык и генерирует целевую речь.

### Результаты

<img src="assets/Studio_metrics.png" width="400">

<img src="assets/Stusio_metrics2.png" width="400">

<img src="assets/Studio_metrics3.png" width="400">
