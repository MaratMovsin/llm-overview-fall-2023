# InstructGPT & RLHF (автор: Азат Валеев)

Как было сказано в предыдущей части, instruction finetuning может быть мощным инструментом для трансформации предобученной на большом корпусе данных языковой модели в полезного ассистента, решающего задачи, написанные человека, быть может, выходящие за рамки видимого им обучающего множества, с которыми он успешно справляется. В данной главе будет рассмотрен подход дообучения на инструкции с использованием **RLHF** (**R**einfocement **L**earning from **H**uman **F**eedback), который является общим, не привязанным к instruction finetuning; который может быть применён для доообучения LLM на классе задач, связанных с так называемым *alignment'ом*: как научить модель быть полезной, безвредной для человека, инкапсулировать в неё человеческие ценности.

Для рассмотрения этого метода была выбрана статья [Training language models to follow instructions
with human feedback](https://arxiv.org/pdf/2203.02155.pdf), результатом работы над которой является семейство моделей **InstructGPT**.

Мотивация авторов статьи состоит в следующем: можно использовать в качестве ассистента LLM, предобученной на корпусе данных из Интернета, однако оптимизируемая цель модели "предсказывать следующий токен на основе контекста" и желаемая цель "сгенерировать ответ, полезный человеку" оказываются несовместимыми как раз из-за природы данных для обучения. Поэтому модель в своих ответах может:

* быть предвзятой ([статья 1](https://arxiv.org/pdf/2101.11718.pdf), [статья 2](https://arxiv.org/pdf/2106.13219.pdf));

* быть стереотипной ([статья](https://arxiv.org/pdf/2004.09456.pdf));

* быть токсичной ([статья](https://arxiv.org/pdf/2009.11462.pdf));

* сливать персональные данные из претрейна ([статья](https://arxiv.org/pdf/2012.07805.pdf));

* генерировать фактически неверную информацию, "галлюцинировать" ([статья](https://arxiv.org/pdf/1908.09203.pdf)).

Для того, чтобы приблизиться к желаемой цели, авторы и применяют для дообучения моделей человеческие предпочтения, которые позволили бы купировать разные вышепредставленные проблемы. 

## InstructGPT: общая схема

Общая схема процесса дообучения проиллюстрована на следующем рисунке.

![Instruct GPT training overview](assets/instruct_gpt_scheme.png)

Тут выделяются три этапа:

1. **S**upervised **f**ine**t**uning, **SFT**, оно же честное дообучение на вручную написанных текстах вида "инструкция -- ответ к ней".

2. **R**eward **M**odel, **RM** -- обучение модели "награды", которая оценивает продолжения текста при условии этого текста.

3. **PPO** -- итоговое дообучение модели для максимизацию награды от **RM**.

Обсудим каждый из этапов подробнее 

## Supervised finetuning (SFT)

На этот шаге берется предобученная модель GPT-3[^1] и дообучается полным finetuning'ом следовать инструкциям: в качестве входа обучаемая сеть принимает вручную написанный разметчиком промпт с инструкцией вида "Напиши историю про лягушку" и должна выдасть его продолжение. Референсное продолжение ("Жила-была лягушка...") также было написано командой разметчиков.

### Данные

Здесь хочется отметить важность подбора данных, которому следовали авторы для этого и следующих этапов. Так для обучения модели с каждого шага были необходимы промпты с инструкцией. Авторы использовали два источника подобного рода данных:

* Работа команды разметчиков. Специально для проекта были наняты разметчики, которые занимались написанием заданий с инструкциями, а в случае SFT и RM шагов они же и писали ответы на эти задания. Авторы тщательно подошли к отбору сотрудников, что описывается в одном из приложений к статье; в процессе работы авторы вели тесный контакт с разметчиками, отвечали на вопросы, собирали обратную связь и улучшали интерфейс разметки. 

* Промпты из API ранней версии InstuctGPT. После дообучения на SFT шаге модель уже хорошо реагировала на запросы с инструкцией, и промпты с инструкциями реальных пользователей к ней из API были использованы для обучения на следующих шагах. Тут авторы также проявили осторожность: пользователи предупреждались о возможном использовании их запросов; с каждого пользователя собиралось ограниченное количество запросов; была произведена фильтрация персональных данных.

От промптов с инструкциями требовалась разнообразность заданий, выделилось два типа промптов: просто промпты, где описывается задание, и few-shot промпты, где необходимо решить задание, используя небольшую разметку, данную в запросе.

## Reinforcement learning

Прежде чем перейти к двум оставшимся этапам, стоит вспомнить о том, что такое обучение с подкреплением и при чём оно здесь. 

Обучение с подкреплением -- это раздел машинного обучения, где обучаемый алгоритм (он часто называется **агентом** в этой науке) взаимодействует с некоторой **средой**. Среда служит источником знаний для агента -- она отдаёт ему свои **состояния** и **награды**, которые являются мерой "хорошести" получения того или иного состояния в процессе взаимодействия. Агент в свою очередь, получая очередное состояние среды и награду, принимает решение о том, какое **действие** ему необходимо сделать в данный момент, и совершает это **действие**, побуждая среду выдать новое состоние и награду. Используя информацию о состояниях и наградах, агент обучается взаимодействовать со средой эффективно, он вырабатывает некоторую **стратегию**. И конечная цель агента -- обучить такую стратегию, чтобы максимизировать награду от своих сделанных действий. 




Можно обучение переформулировать 

## Reward Model


[^1]: Здесь и далее в тексте в качестве предобученной модели брали GPT-3  с различным числом обучаемых параметром