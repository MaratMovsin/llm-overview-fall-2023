# Обучение нейронных сетей с математической точки зрения

## Forward & backward pass в нейронных сетях (автор: Азат Валеев)

## SGD и прочие оптимизаторы (автор: Александра Павлова)

## Анализ памяти для наивного шага обучения (автор: Азат Валеев)

# Оптимизации

## Модификации оптимизатора по памяти: Adafactor, 8-bit Adam (автор: Александра Павлова)

## Gradient Accumulation (автор: Азат Валеев)

## Activation Checkpointing (автор: Азат Валеев)

## Mixed Precision Training (автор: Георгий Ангени)

Часто встречающимися типами значений для представления нейронных моделей в памяти являются FP32 и FP16 – форматы значений с плавающей точкой, занимающие в памяти 4 и 2 байта соответственно. Ясно, что использование FP16 вместо FP32 позволит уменьшить объем памяти, занимаемый моделью, и увеличить скорость ее обучения, но оно также приведет к ограничению точности параметров, которые модель сможет выучить.

Ниже проиллюстрированы представления FP32 и FP16 в памяти: $S$ – бит знака, $M$ – мантисса (часть числа с плавающей точкой) в двоичном виде, которая умножается на $2^E$, где $E$ – экспонента, представляющая собой целое число в двоичном виде.

![Bitwise representation](assets/mixed_precision_1.png)

В Mixed Precision Training в процессе обучения для весов, значений активации и градиентов используется формат FP16, но сама модель, или ее так называемые master weights, хранятся в FP32. Соответственно, веса, используемые при обучении, получаются именно путем конвертации master weights в FP16.

![Mixed Precision Training](assets/mixed_precision_2.png)

Почему же мы не храним все веса модели в FP16 изначально? Заметим, что разница между экспонентами двух чисел в FP16 (так же, как и в FP32) на единицу соответствует битовому сдвигу мантиссы. Это в сущности означает, что при складывании двух чисел в FP16, отличающихся друг от друга в $2048$ раз (число $2^11$, которое приведет к битовому сдвигу на 11 бит) меньшее число будет обращено в ноль, что сильно скажется на потенциальной точности, которую могут достичь параметры модели при обучении.

Также для того, чтобы значения градиентов в FP16 при обратном проходе не были приравнены к нулю, значение функции потерь искусственно увеличивается в несколько раз и обратное преобразование осуществляется сразу после backward pass (операция называется loss scaling) для того, чтобы не пришлось изменять ранее подобранные гиперпараметры, связанные с градиентами, такие как gradient clipping threshold. На изображении ниже показано распределение изменения весов модели на одной итерации обучения и видно, что без loss scaling все обновления левее красной полосы будут обращены в ноль. Увеличение значения функции потерь даже в 8 раз приведет к тому, что будет учтено сильно больше обновлений и модель обучится лучше.

![Mixed Precision Training](assets/mixed_precision_3.png)
